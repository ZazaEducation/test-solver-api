"""Test-related data models."""

from datetime import datetime
from enum import Enum
from typing import List, Optional
from uuid import UUID

from pydantic import BaseModel, Field, validator


class TestStatus(str, Enum):
    """Test processing status."""
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"


class QuestionType(str, Enum):
    """Question types supported by the system."""
    MULTIPLE_CHOICE = "multiple_choice"
    SHORT_ANSWER = "short_answer"
    ESSAY = "essay"
    TRUE_FALSE = "true_false"
    FILL_BLANK = "fill_blank"
    OTHER = "other"


class QuestionCreate(BaseModel):
    """Model for creating a question."""
    test_id: UUID
    question_number: int = Field(..., ge=1, description="Question number (1-based)")
    question_text: str = Field(..., min_length=1, description="The original text of the question")
    question_type: QuestionType = Field(..., description="The type of the question")
    options: List[str] = Field(default_factory=list, description="List of options for multiple-choice questions")
    
    @validator('options')
    def validate_options(cls, v, values):
        """Validate that multiple choice questions have options."""
        if values.get('question_type') == QuestionType.MULTIPLE_CHOICE and not v:
            raise ValueError("Multiple choice questions must have options")
        return v


class QuestionUpdate(BaseModel):
    """Model for updating a question with AI-generated answer."""
    ai_answer: str = Field(..., min_length=1, description="The answer generated by the AI")
    confidence: float = Field(..., ge=0.0, le=1.0, description="AI's confidence level (0 to 1)")
    explanation: str = Field(..., min_length=1, description="Detailed explanation of the AI's answer")
    processing_time: Optional[float] = Field(None, ge=0, description="Time taken to process this question")


class QuestionResponse(BaseModel):
    """Model for question response."""
    question_number: int = Field(..., description="The number assigned to the question")
    question_text: str = Field(..., description="The original text of the question")
    question_type: QuestionType = Field(..., description="The type of the question")
    options: List[str] = Field(default_factory=list, description="List of options for multiple-choice questions")
    ai_answer: Optional[str] = Field(None, description="The answer generated by the AI")
    confidence: Optional[float] = Field(None, ge=0.0, le=1.0, description="AI's confidence level (0 to 1)")
    explanation: Optional[str] = Field(None, description="Detailed explanation of the AI's answer")
    
    class Config:
        from_attributes = True


class TestCreate(BaseModel):
    """Model for creating a test."""
    title: str = Field(..., min_length=1, max_length=255, description="Title or name of the test")
    original_filename: str = Field(..., min_length=1, description="Original name of the uploaded file")
    created_by: str = Field(..., description="Email of the user who created the record")


class TestUpdate(BaseModel):
    """Model for updating a test."""
    status: Optional[TestStatus] = None
    processing_time: Optional[float] = Field(None, ge=0, description="Time taken to process the test")
    total_questions: Optional[int] = Field(None, ge=0, description="Total number of questions found")
    error_message: Optional[str] = None


class TestResponse(BaseModel):
    """Model for test response - matches the exact JSON schema required."""
    id: str = Field(..., description="Unique identifier for the test")
    created_date: datetime = Field(..., description="Timestamp when the test record was created")
    updated_date: datetime = Field(..., description="Timestamp when the test record was last updated")
    created_by: str = Field(..., description="Email of the user who created the record")
    title: str = Field(..., description="Title or name of the test")
    file_url: str = Field(..., description="URL of the uploaded test file")
    original_filename: str = Field(..., description="Original name of the uploaded file")
    status: TestStatus = Field(default=TestStatus.PROCESSING, description="Current processing status")
    questions: List[QuestionResponse] = Field(
        default_factory=list,
        description="Extracted questions with AI-generated answers and explanations"
    )
    processing_time: Optional[float] = Field(None, description="Time taken to process the test in seconds")
    total_questions: int = Field(default=0, description="Total number of questions found in the test")
    
    class Config:
        from_attributes = True


class ProcessingJobCreate(BaseModel):
    """Model for creating a processing job."""
    test_id: UUID
    job_type: str = Field(..., description="Type of processing job")
    metadata: dict = Field(default_factory=dict, description="Additional job metadata")


class ProcessingJobResponse(BaseModel):
    """Model for processing job response."""
    id: UUID
    test_id: UUID
    job_type: str
    status: str
    started_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None
    error_message: Optional[str] = None
    metadata: dict = Field(default_factory=dict)
    created_date: datetime
    
    class Config:
        from_attributes = True